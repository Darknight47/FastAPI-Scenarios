{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fee62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb80e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": \"../dataset/gpt2_recipes.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1dc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "Name: Low-Fat Berry Blue Frozen Dessert\n",
      "Ingredients: blueberries, granulated sugar, vanilla yogurt, lemon juice\n",
      "Instructions:\n",
      "Toss 2 cups berries with sugar.\n",
      "Let stand for 45 minutes\n",
      "stirring occasionally.\n",
      "Transfer berry-sugar mixture to food processor.\n",
      "Add yogurt and process until smooth.\n",
      "Strain through fine sieve. Pour into baking pan (or transfer to ice cream maker and process according to manufacturers' directions). Freeze uncovered until edges are solid but centre is soft.  Transfer to processor and blend until smooth again.\n",
      "Return to pan and freeze until edges are solid.\n",
      "Transfer to processor and blend until smooth again.\n",
      "\n",
      "Fold in remaining 2 cups of blueberries.\n",
      "Pour into plastic mold and freeze overnight. Let soften slightly to serve.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows\n",
    "for i in range(16):\n",
    "    print(dataset[\"train\"][i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4025ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add special tokens if you're using them\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'eos_token': '<|endoftext|>',\n",
    "})\n",
    "\n",
    "# Set pad token (needed for padding during batching)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcdc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c50d8",
   "metadata": {},
   "source": [
    "Transforms each sample from raw text into token IDs  \n",
    "Stores the result as a new Dataset object ready for training  \n",
    "Language models like GPT-2 don’t understand raw text like: \"Ingredients: tomato, onion, garlic\"  \n",
    "They require that text to be tokenized into numbers (IDs), using the tokenizer that matches the model.  \n",
    "tokenizer.encode(\"tomato\")  → [15496]  \n",
    "\n",
    "> tokenization is how we turn our cleaned recipe text into input the model can understand and train on.\n",
    "\n",
    "What we've done so far:\n",
    "1. GPT-2 needs numbers, not text, Converts the text into token IDs\n",
    "2. GPT-2 can't handle long inputs, Truncates to max 512 tokens\n",
    "3. Trainer needs equal-size inputs,\tPads short ones to same length\n",
    "4. GPT-2 doesn’t have pad token, Assigns `<\n",
    "5. Model needs start/end markers, Adds custom special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c47b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed57124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just taking 1000 samples for quicker training in this example\n",
    "sample_dataset = tokenized_dataset[\"train\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd03968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Resize embedding layer to include new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# This makes sure the model understands your added tokens like <|startoftext|>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496d9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We are NOT using masked language modeling (BERT style)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e23f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../NLPModel/gpt2-recipes\",         # where to save the model\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                  # try 1 first to test\n",
    "    per_device_train_batch_size=2,       # lower if memory is tight\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df26adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhad\\AppData\\Local\\Temp\\ipykernel_20916\\336921182.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sample_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57455c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 50257, 'pad_token_id': 50256}.\n",
      "f:\\CODE\\FastAPI-Scenarios\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 53:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.961300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\CODE\\FastAPI-Scenarios\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.5535806477864584, metrics={'train_runtime': 3212.9717, 'train_samples_per_second': 0.467, 'train_steps_per_second': 0.233, 'total_flos': 391938048000000.0, 'train_loss': 2.5535806477864584, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
